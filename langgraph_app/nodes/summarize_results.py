import openai
import json
from typing import List, Dict
import os

SUMMARIZER_PROMPTS = {
    "price_movement_analysis": """
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è­‰åˆ¸åˆ†æå¸«ï¼Œè«‹æ ¹æ“šè¼¸å…¥çš„å…¬å¸ä»£è™Ÿèˆ‡è¿‘æœŸå¸‚å ´è®ŠåŒ–ï¼Œåˆ†æè©²è‚¡ç¥¨åœ¨ç‰¹å®šæ—¥æœŸç™¼ç”Ÿæ¼²åœã€è·Œåœæˆ–åŠ‡çƒˆè®Šå‹•çš„å¯èƒ½åŸå› ã€‚

è«‹æ•´åˆä»¥ä¸‹è³‡è¨Šä¾†æºï¼ˆè‹¥æœ‰ï¼‰ä¸¦æ¢åˆ—åˆ†æï¼š
1. æ³•äººå‹•å‘ï¼ˆå¤–è³‡ã€æŠ•ä¿¡ã€è‡ªç‡Ÿå•†ï¼‰
2. å¤§æˆ¶è²·è³£æˆ–ä¸»åŠ›é€²å‡º
3. æŠ€è¡“é¢è®ŠåŒ–ï¼ˆå¦‚å‡ç·šäº¤å‰ã€æ”¯æ’å£“åŠ›ã€éç†±ï¼‰
4. æ–°èé¢èˆ‡äº‹ä»¶é©…å‹•ï¼ˆETFæ›è‚¡ã€æ³•èªªæœƒã€è²¡å ±ã€åˆ©å¤šåˆ©ç©ºï¼‰
5. æ•´é«”ç›¤å‹¢èˆ‡è³‡é‡‘æµå‹•å½±éŸ¿

ç¦æ­¢ä¸»è§€é æ¸¬ï¼Œè«‹ä¿æŒä¸­ç«‹ã€è³‡è¨Šå®¢è§€ã€‚
""",

    "financial_summary": """
è«‹æ•´ç†å¯æŸ¥å¾—çš„è²¡å‹™æ•¸æ“šï¼ŒåŒ…æ‹¬ï¼š
- æ¯è‚¡ç›ˆé¤˜ (EPS)ï¼šæœ€æ–°å­£åº¦ã€å¹´å¢ç‡ã€å­£å¢ç‡è¶¨å‹¢
- ç‡Ÿæ”¶è¡¨ç¾ï¼šæœ€æ–°ç‡Ÿæ”¶ã€å¹´å¢ç‡ã€å­£å¢ç‡è¶¨å‹¢
- æç›Šè¡¨é‡é»ï¼šç‡Ÿæ¥­æ”¶å…¥ã€ç‡Ÿæ¥­æ¯›åˆ©ã€ç‡Ÿæ¥­åˆ©ç›Šã€ç¨…å¾Œæ·¨åˆ©
- è³‡ç”¢è² å‚µè¡¨é‡é»ï¼šè³‡ç”¢ç¸½è¨ˆã€è² å‚µç¸½è¨ˆã€è‚¡æ±æ¬Šç›Š
- è²¡å‹™æ¯”ç‡ï¼šæ¯›åˆ©ç‡ã€ç‡Ÿæ¥­åˆ©ç›Šç‡ã€æ·¨åˆ©ç‡ã€ROEã€ROA
- ç¾é‡‘æµé‡ï¼šç‡Ÿæ¥­ç¾é‡‘æµã€æŠ•è³‡ç¾é‡‘æµã€ç±Œè³‡ç¾é‡‘æµ

å¦‚æœæ²’æœ‰ Yahoo è²¡ç¶“æ•¸æ“šï¼Œè«‹å¾æ–°èå…§å®¹ä¸­æå–ç›¸é—œè²¡å‹™è³‡è¨Šã€‚

è«‹ä»¥ã€Œæ ¹æ“š Yahoo è²¡ç¶“è²¡å‹™å ±è¡¨ã€æˆ–ã€Œæ ¹æ“šå…¬é–‹è³‡æ–™ã€èªæ°£å‘ˆç¾ï¼Œä¸åšä»»ä½•æŠ•è³‡å»ºè­°ã€‚
""",

    "industry_context": """
è«‹èªªæ˜è©²è‚¡ç¥¨æ‰€å±¬ç”¢æ¥­çš„è¿‘æœŸè¶¨å‹¢èˆ‡ç’°å¢ƒæ¦‚æ³ï¼ŒåŒ…å«ï¼š
- è¿‘æœŸç†±é–€é¡Œæï¼ˆå¦‚ AIã€ç”ŸæŠ€ã€é‡é›»ç­‰ï¼‰
- åŒç”¢æ¥­ç«¶çˆ­å°æ‰‹æ¦‚æ³
- å¤§ç›¤èˆ‡ç¸½é«”ç¶“æ¿Ÿè®Šæ•¸ï¼ˆå¦‚åˆ©ç‡ã€åŸç‰©æ–™åƒ¹æ ¼ã€å¤–éŠ·ï¼‰

è«‹ç¶­æŒå®¢è§€ã€ä¸è©•è«–è²·è³£ã€‚
""",

    "analyst_estimate": """
è«‹æ•´ç†å¯æŸ¥å¾—çš„åˆ†æå¸«é ä¼°æ•¸æ“šï¼ŒåŒ…æ‹¬ï¼š
- ä»Šå¹´åº¦(2025) EPS é ä¼°å€¼ï¼šä¸­ä½æ•¸ã€æœ€é«˜å€¼ã€æœ€ä½å€¼ã€åˆ†æå¸«äººæ•¸
- æ˜å¹´åº¦(2026) EPS é ä¼°å€¼ï¼šä¸­ä½æ•¸ã€æœ€é«˜å€¼ã€æœ€ä½å€¼ã€åˆ†æå¸«äººæ•¸  
- ç›®æ¨™åƒ¹ï¼šä¸­ä½æ•¸ã€æœ€é«˜å€¼ã€æœ€ä½å€¼ã€åˆ†æå¸«äººæ•¸
- æŠ•è³‡å»ºè­°åˆ†å¸ƒï¼šè²·é€²ã€æŒæœ‰ã€è³£å‡ºæ¯”ä¾‹
- å¸‚å ´å…±è­˜èªæ°£ï¼ˆå¦‚åå¤šã€ä¸­æ€§ã€åç©ºï¼‰

å¦‚æœæ²’æœ‰åˆ†æå¸«é ä¼°æ•¸æ“šï¼Œè«‹å¾æ–°èå…§å®¹ä¸­æå–ç›¸é—œåˆ†æå¸«é ä¼°è³‡è¨Šã€‚

è«‹ä»¥ã€Œæ ¹æ“šå¸‚å ´åˆ†æå¸«é ä¼°ã€æˆ–ã€Œæ ¹æ“šå…¬é–‹è³‡æ–™ã€èªæ°£å‘ˆç¾ï¼Œä¸åšä»»ä½•æŠ•è³‡å»ºè­°ã€‚
""",

    "investment_strategy_by_time": """
é‡å°ä¸åŒæŠ•è³‡æœŸé–“ï¼Œè«‹èªªæ˜è©²è‚¡ç¥¨åœ¨ä¸åŒæ™‚é–“æ¶æ§‹ä¸­ï¼ŒæŠ•è³‡è€…å¯è§€å¯Ÿçš„è®Šæ•¸èˆ‡å¯èƒ½æ€è€ƒè§’åº¦ï¼š

- ğŸ“‰ æ—¥å…§ï¼ˆç•¶æ²–ã€çŸ­ç·šï¼‰ï¼šé‡åƒ¹ç•°å‹•ã€æŠ€è¡“å‹æ…‹
- ğŸ”„ çŸ­ç·šï¼ˆ1â€“5 æ—¥ï¼‰ï¼šæ¶ˆæ¯é¢æ•ˆæ‡‰ã€ä¸»åŠ›ç±Œç¢¼è¡Œç‚º
- ğŸ“… ä¸­æœŸï¼ˆ1â€“3 é€±ï¼‰ï¼šæ³•äººä½ˆå±€ã€é¡Œæåæ‡‰ã€æ³•èªªæœƒ
- ğŸ§­ é•·æœŸï¼ˆ1â€“3 æœˆä»¥ä¸Šï¼‰ï¼šåŸºæœ¬é¢è¶¨å‹¢ã€EPS é ä¼°ã€ç”¢æ¥­è¶¨å‹¢

æä¾›æ€è€ƒä¾æ“šï¼Œä¸çµ¦äºˆæ˜ç¢ºæ“ä½œå»ºè­°ã€‚
""",

    "action_recommendation": """
æ ¹æ“šä¸Šè¿°åˆ†æï¼Œè«‹ä»¥ç°¡è¦æ–¹å¼åˆ—å‡ºå°ä¸åŒé¡å‹æŠ•è³‡äººï¼ˆå¦‚çŸ­ç·šäº¤æ˜“è€…ã€é•·ç·šæŒæœ‰è€…ï¼‰å¯è€ƒæ…®çš„ç­–ç•¥æ–¹å‘ï¼Œä¸¦èªªæ˜åŸå› ã€‚

èˆ‰ä¾‹æ ¼å¼ï¼š
- çŸ­ç·šå»ºè­°ï¼šä¿å®ˆè§€æœ› / æŠ€è¡“æ•´ç†ä¸­
- ä¸­æœŸè§€é»ï¼šä¸­æ€§åå¤šï¼Œé—œæ³¨ XXX æŒ‡æ¨™
- é•·ç·šçœ‹æ³•ï¼šåŸºæœ¬é¢ä»ç©©å¥ï¼Œä½†å»ºè­°é€¢å›ä½ˆå±€

å»ºè­°èªæ°£éœ€è¬¹æ…ï¼Œä¿æŒè³‡è¨Šä¸­ç«‹èˆ‡ä¿ç•™ç©ºé–“ã€‚
""",

    "risk_assessment": """
è«‹æ¢åˆ—è©²è‚¡ç¥¨å¯èƒ½é¢è‡¨çš„æŠ•è³‡é¢¨éšªèˆ‡è§€å¯Ÿé»ï¼ŒåŒ…æ‹¬ä½†ä¸é™æ–¼ï¼š
- ç”¢æ¥­ç«¶çˆ­é¢¨éšª
- ç²åˆ©ä¸å¦‚é æœŸé¢¨éšª
- æ³•äººç±Œç¢¼è½‰å¼±é¢¨éšª
- æŠ€è¡“é¢è½‰ç©ºæˆ–è·Œç ´æ”¯æ’
- å¤§ç›¤æˆ–ç¸½é«”ç’°å¢ƒæƒ¡åŒ–é¢¨éšª

è«‹ç›¡é‡è²¼è¿‘è©²è‚¡ç¥¨å¯¦éš›æƒ…å¢ƒã€‚
""",

    "disclaimer": """
å…è²¬è²æ˜ï¼š
æœ¬å ±å‘Šåƒ…ä¾›åƒè€ƒï¼Œä¸æ§‹æˆæŠ•è³‡å»ºè­°ã€‚æŠ•è³‡äººæ‡‰è‡ªè¡Œæ‰¿æ“”æŠ•è³‡é¢¨éšªï¼Œä¸¦åœ¨æŠ•è³‡å‰å……åˆ†äº†è§£ç›¸é—œé¢¨éšªã€‚éå¾€ç¸¾æ•ˆä¸ä»£è¡¨æœªä¾†è¡¨ç¾ï¼ŒæŠ•è³‡æœ‰è³ºæœ‰è³ ï¼ŒæŠ•è³‡äººæ‡‰å¯©æ…è©•ä¼°ã€‚
""",

    "data_sources": """
è³‡æ–™ä¾†æºï¼š
è«‹æ•´ç†æœ¬å ±å‘Šä½¿ç”¨çš„æ‰€æœ‰è³‡æ–™ä¾†æºï¼ŒåŒ…æ‹¬ï¼š
- Yahoo è²¡ç¶“è²¡å‹™å ±è¡¨é€£çµ
- æ–°èä¾†æº
- å…¶ä»–å…¬é–‹è³‡æ–™ä¾†æº

è«‹ä»¥ã€Œæœ¬å ±å‘Šè³‡æ–™ä¾†æºï¼šã€é–‹é ­ï¼Œåˆ—å‡ºæ‰€æœ‰ä½¿ç”¨çš„è³‡æ–™é€£çµã€‚
"""
}

def summarize_results(company_name: str, stock_id: str, news_results: List[Dict], user_input: str, factset_data: str = "", news_sources: List[Dict] = None, financial_sources: List[Dict] = None) -> Dict:
    """
    ä½¿ç”¨ OpenAI ç”ŸæˆæŠ•è³‡åˆ†æå ±å‘Šæ‘˜è¦
    """
    try:
        client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # æº–å‚™æ–°èå…§å®¹
        news_content = ""
        for i, news in enumerate(news_results[:20]):  # é™åˆ¶å‰20å‰‡æ–°è
            title = news.get("title", "")
            snippet = news.get("snippet", "")
            news_content += f"æ–°è{i+1}: {title}\n{snippet}\n\n"
        
        # æ§‹å»ºå®Œæ•´çš„æç¤ºè©
        full_prompt = f"""
è«‹æ ¹æ“šä»¥ä¸‹è³‡è¨Šï¼Œç‚º {company_name}({stock_id}) ç”Ÿæˆä¸€ä»½è©³ç´°çš„æŠ•è³‡åˆ†æå ±å‘Šã€‚

ç”¨æˆ¶å•é¡Œï¼š{user_input}

{factset_data}

ç›¸é—œæ–°èï¼š
{news_content}

è«‹æŒ‰ç…§ä»¥ä¸‹8å€‹é¢å‘ç”Ÿæˆåˆ†æå ±å‘Šï¼š

1. ğŸ“ˆ è‚¡åƒ¹è®Šå‹•åˆ†æ
{SUMMARIZER_PROMPTS['price_movement_analysis']}

2. ğŸ“Š è²¡å‹™æ•¸æ“šæ‘˜è¦
{SUMMARIZER_PROMPTS['financial_summary']}

3. ğŸŒ ç”¢æ¥­ç’°å¢ƒåˆ†æ
{SUMMARIZER_PROMPTS['industry_context']}

4. ğŸ¯ åˆ†æå¸«é ä¼°
{SUMMARIZER_PROMPTS['analyst_estimate']}

5. â° æŠ•è³‡ç­–ç•¥å»ºè­°
{SUMMARIZER_PROMPTS['investment_strategy_by_time']}

6. ğŸ’¡ æ“ä½œå»ºè­°
{SUMMARIZER_PROMPTS['action_recommendation']}

7. âš ï¸ é¢¨éšªè©•ä¼°
{SUMMARIZER_PROMPTS['risk_assessment']}

è«‹ç¢ºä¿æ¯å€‹é¢å‘éƒ½æœ‰è©³ç´°çš„åˆ†æå…§å®¹ã€‚
"""
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è­‰åˆ¸åˆ†æå¸«ï¼Œæ“…é•·åˆ†æå°è‚¡å€‹è‚¡ã€‚"},
                {"role": "user", "content": full_prompt}
            ],
            temperature=0.7,
            max_tokens=4000
        )
        
        summary = response.choices[0].message.content
        
        # è§£ææ‘˜è¦ç‚ºä¸åŒçš„ sections
        sections = parse_summary_sections(summary)
        
        # æ·»åŠ è³‡æ–™ä¾†æº section
        if news_sources or financial_sources:
            sources_content = []
            
            if news_sources and isinstance(news_sources, list):
                sources_content.append("ğŸ“° æ–°èè³‡æ–™ä¾†æºï¼š")
                for source in news_sources:
                    if isinstance(source, dict):
                        title = source.get('title', 'ç„¡æ¨™é¡Œ')
                        link = source.get('link', '')
                        if link:
                            sources_content.append(f"â€¢ <a href='{link}' target='_blank'>{title}</a>")
                        else:
                            sources_content.append(f"â€¢ {title}")
                sources_content.append("")
            
            if financial_sources and isinstance(financial_sources, list):
                sources_content.append("ğŸ“Š è²¡å‹™è³‡æ–™ä¾†æºï¼š")
                for source in financial_sources:
                    if isinstance(source, dict):
                        name = source.get('name', 'æœªçŸ¥ä¾†æº')
                        url = source.get('url', '')
                        if url:
                            sources_content.append(f"â€¢ <a href='{url}' target='_blank'>{name}</a>")
                        else:
                            sources_content.append(f"â€¢ {name}")
            
            sections["ğŸ“š è³‡æ–™ä¾†æº"] = '\n'.join(sources_content)
        
        # æ·»åŠ å…è²¬è²æ˜ section
        disclaimer_content = """æœ¬å ±å‘Šåƒ…ä¾›åƒè€ƒï¼Œä¸æ§‹æˆæŠ•è³‡å»ºè­°ã€‚æŠ•è³‡äººæ‡‰è‡ªè¡Œæ‰¿æ“”æŠ•è³‡é¢¨éšªï¼Œä¸¦åœ¨æŠ•è³‡å‰å……åˆ†äº†è§£ç›¸é—œé¢¨éšªã€‚

â€¢ æœ¬å ±å‘Šæ‰€è¼‰è³‡æ–™åƒ…ä¾›åƒè€ƒï¼Œä¸ä¿è­‰å…¶æº–ç¢ºæ€§ã€å®Œæ•´æ€§æˆ–æ™‚æ•ˆæ€§
â€¢ æŠ•è³‡æœ‰é¢¨éšªï¼Œå…¥å¸‚éœ€è¬¹æ…ï¼Œéå¾€è¡¨ç¾ä¸ä»£è¡¨æœªä¾†è¡¨ç¾
â€¢ æŠ•è³‡äººæ‡‰æ ¹æ“šè‡ªèº«é¢¨éšªæ‰¿å—èƒ½åŠ›åšå‡ºæŠ•è³‡æ±ºç­–
â€¢ æœ¬å ±å‘Šä¸æ§‹æˆä»»ä½•æŠ•è³‡å»ºè­°æˆ–æ¨è–¦"""
        
        sections["âš–ï¸ å…è²¬è²æ˜"] = disclaimer_content
        
        return {
            "success": True,
            "sections": sections,
            "raw_summary": summary
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

def parse_summary_sections(summary: str) -> Dict[str, str]:
    """
    è§£ææ‘˜è¦å…§å®¹ç‚ºä¸åŒçš„ sections
    """
    sections = {}
    current_section = ""
    current_content = []
    
    lines = summary.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # æª¢æŸ¥æ˜¯å¦ç‚º section æ¨™é¡Œï¼ˆæ›´å¯¬é¬†çš„åŒ¹é…ï¼‰
        if (any(keyword in line for keyword in ['ğŸ“ˆ', 'ğŸ“Š', 'ğŸŒ', 'ğŸ¯', 'â°', 'ğŸ’¡', 'âš ï¸']) or
            any(keyword in line for keyword in ['è‚¡åƒ¹è®Šå‹•åˆ†æ', 'è²¡å‹™æ•¸æ“šæ‘˜è¦', 'ç”¢æ¥­ç’°å¢ƒåˆ†æ', 'åˆ†æå¸«é ä¼°', 'æŠ•è³‡ç­–ç•¥å»ºè­°', 'æ“ä½œå»ºè­°', 'é¢¨éšªè©•ä¼°']) or
            (line.startswith(('1.', '2.', '3.', '4.', '5.', '6.', '7.')) and any(keyword in line for keyword in ['ğŸ“ˆ', 'ğŸ“Š', 'ğŸŒ', 'ğŸ¯', 'â°', 'ğŸ’¡', 'âš ï¸']))):
            
            # ä¿å­˜å‰ä¸€å€‹ section
            if current_section and current_content:
                sections[current_section] = '\n'.join(current_content).strip()
            
            # é–‹å§‹æ–°çš„ section
            current_section = line
            current_content = []
        else:
            # æ·»åŠ åˆ°ç•¶å‰ section å…§å®¹
            if current_section:
                current_content.append(line)
    
    # ä¿å­˜æœ€å¾Œä¸€å€‹ section
    if current_section and current_content:
        sections[current_section] = '\n'.join(current_content).strip()
    
    # å¦‚æœæ²’æœ‰è§£æåˆ°ä»»ä½• sectionï¼Œå˜—è©¦æ‰‹å‹•åˆ†å‰²
    if not sections and summary:
        # æ ¹æ“šé—œéµå­—æ‰‹å‹•åˆ†å‰²
        section_keywords = [
            ('ğŸ“ˆ è‚¡åƒ¹è®Šå‹•åˆ†æ', 'ğŸ“ˆ'),
            ('ğŸ“Š è²¡å‹™æ•¸æ“šæ‘˜è¦', 'ğŸ“Š'),
            ('ğŸŒ ç”¢æ¥­ç’°å¢ƒåˆ†æ', 'ğŸŒ'),
            ('ğŸ¯ åˆ†æå¸«é ä¼°', 'ğŸ¯'),
            ('â° æŠ•è³‡ç­–ç•¥å»ºè­°', 'â°'),
            ('ğŸ’¡ æ“ä½œå»ºè­°', 'ğŸ’¡'),
            ('âš ï¸ é¢¨éšªè©•ä¼°', 'âš ï¸')
        ]
        
        for section_title, emoji in section_keywords:
            if emoji in summary:
                # æ‰¾åˆ°è©² section çš„é–‹å§‹ä½ç½®
                start_idx = summary.find(emoji)
                if start_idx != -1:
                    # æ‰¾åˆ°ä¸‹ä¸€å€‹ section çš„é–‹å§‹ä½ç½®
                    next_start = -1
                    for next_emoji in ['ğŸ“ˆ', 'ğŸ“Š', 'ğŸŒ', 'ğŸ¯', 'â°', 'ğŸ’¡', 'âš ï¸']:
                        if next_emoji != emoji:
                            next_idx = summary.find(next_emoji, start_idx + 1)
                            if next_idx != -1 and (next_start == -1 or next_idx < next_start):
                                next_start = next_idx
                    
                    # æå– section å…§å®¹
                    if next_start != -1:
                        section_content = summary[start_idx:next_start].strip()
                    else:
                        section_content = summary[start_idx:].strip()
                    
                    # ç§»é™¤æ¨™é¡Œï¼Œåªä¿ç•™å…§å®¹
                    lines = section_content.split('\n')
                    if lines and emoji in lines[0]:
                        content_lines = lines[1:]
                        sections[section_title] = '\n'.join(content_lines).strip()
                    else:
                        sections[section_title] = section_content
    
    return sections

# æ¸¬è©¦ç”¨
if __name__ == "__main__":
    test_results = [
        {
            "title": "è¯ç¢©è‚¡åƒ¹åˆ†æ",
            "snippet": "è¯ç¢©(2357)è¿‘æœŸè‚¡åƒ¹è¡¨ç¾å¼·å‹ï¼Œä¸»è¦å—æƒ æ–¼AI PCéœ€æ±‚æˆé•·...",
            "link": "https://example.com/news1"
        },
        {
            "title": "è¯ç¢©è²¡å ±äº®çœ¼",
            "snippet": "è¯ç¢©æœ€æ–°è²¡å ±é¡¯ç¤ºEPSæˆé•·15%ï¼Œç‡Ÿæ”¶å‰µæ–°é«˜...",
            "link": "https://example.com/news2"
        }
    ]
    
    result = summarize_results("è¯ç¢©", "2357", test_results, "è¯ç¢©è‚¡åƒ¹åˆ†æ")
    print(json.dumps(result, ensure_ascii=False, indent=2)) 