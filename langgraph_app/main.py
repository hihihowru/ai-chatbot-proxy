from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect, Response, Query
from pydantic import BaseModel
from langgraph_app.nodes.detect_stock import detect_stocks
from langgraph_app.nodes.detect_time import detect_time
from langgraph_app.nodes.detect_chart import detect_chart
from langgraph_app.data_tools.database_query import db_query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
import json
from typing import List, Dict
import time
import os
from dotenv import load_dotenv
from langgraph_app.nodes.classify_and_extract import classify_and_extract
from langgraph_app.nodes.search_news import search_news_smart
from langgraph_app.nodes.generate_report_pipeline import generate_report_pipeline
import openai
from bs4 import BeautifulSoup
import requests
import yfinance as yf
import pandas as pd
from routes.answer import router as answer_router

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

app = FastAPI()

# å…è¨±è·¨åŸŸï¼Œæ–¹ä¾¿æœ¬åœ°å‰ç«¯é–‹ç™¼
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åŒ…å« answer è·¯ç”±
app.include_router(answer_router, prefix="/api")

# è¿½è¹¤æ‰€æœ‰æ´»èºçš„ WebSocket é€£ç·š
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def send_message(self, message: str, websocket: WebSocket):
        await websocket.send_text(json.dumps({"log": message}))

manager = ConnectionManager()

class AskRequest(BaseModel):
    question: str

class DatabaseTestRequest(BaseModel):
    table_ids: List[str]
    stock_id: str = "2330"

class DatabaseQueryRequest(BaseModel):
    intent_category: str
    investment_aspect: str
    stock_ids: List[str]

class ChartQueryRequest(BaseModel):
    stock_ids: List[str]
    chart_type: str

class InvestmentAnalysisRequest(BaseModel):
    question: str
    serper_api_key: str = None

@app.post("/api/ask")
async def ask_api(req: AskRequest):
    stock_ids = detect_stocks(req.question)
    return {"stock_ids": stock_ids}

@app.post("/api/ask-logs")
async def ask_logs_api(req: AskRequest):
    logs = []
    logs.append("ğŸ” é–‹å§‹åµæ¸¬è‚¡ç¥¨ä»£è™Ÿ...")
    stock_ids = detect_stocks(req.question)
    if stock_ids:
        logs.append(f"âœ… åµæ¸¬åˆ°è‚¡ç¥¨ä»£è™Ÿï¼š{', '.join(stock_ids)}")
    else:
        logs.append("âŒ æœªåµæ¸¬åˆ°è‚¡ç¥¨ä»£è™Ÿ")
    return {"logs": logs}

@app.get("/api/ask-sse")
def ask_sse_api(question: str = Query(...)):
    def event_stream():
        try:
            # 1. å•é¡Œç†è§£èˆ‡é—œéµè³‡è¨Šæå–
            yield f"data: {json.dumps({'log': 'ğŸ§  å•é¡Œç†è§£èˆ‡é—œéµè³‡è¨Šæå–ä¸­...'})}\n\n"
            classify_result = classify_and_extract(question)
            yield f"data: {json.dumps({'log': 'ğŸ§  å•é¡Œç†è§£çµæœ: ' + json.dumps(classify_result, ensure_ascii=False)})}\n\n"
            #time.sleep(0.5)

            # 2. å¤šè‚¡è™Ÿåµæ¸¬
            yield f"data: {json.dumps({'log': 'ğŸ” è‚¡ç¥¨ä»£è™Ÿåµæ¸¬ä¸­...'})}\n\n"
            stock_ids = detect_stocks(question)
            yield f"data: {json.dumps({'log': 'ğŸ” åµæ¸¬åˆ°è‚¡ç¥¨ä»£è™Ÿ: ' + (', '.join(stock_ids) if stock_ids else 'æœªåµæ¸¬åˆ°è‚¡ç¥¨')})}\n\n"
            #time.sleep(0.5)

            # 3. æ™‚é–“åµæ¸¬
            yield f"data: {json.dumps({'log': 'â³ æ™‚é–“åµæ¸¬ä¸­...'})}\n\n"
            time_info = detect_time(question)
            yield f"data: {json.dumps({'log': 'â±ï¸ åµæ¸¬åˆ°æ™‚é–“: ' + str(time_info)})}\n\n"
            #time.sleep(0.5)

            # 4. å•é¡Œåˆ†é¡ï¼ˆå¤§åˆ†é¡/å­åˆ†é¡/é¢å‘ï¼‰
            yield f"data: {json.dumps({'log': 'ğŸ§  å•é¡Œåˆ†é¡ä¸­...'})}\n\n"
            category_result = classify_result.get("category", "")
            subcategory_result = classify_result.get("subcategory", [])
            view_type_result = classify_result.get("view_type", [])
            yield f"data: {json.dumps({'log': f'ğŸ“Š å•é¡Œåˆ†é¡çµæœ: å¤§åˆ†é¡={category_result}, å­åˆ†é¡={subcategory_result}, æŠ•è³‡é¢å‘={view_type_result}'})}\n\n"
            #time.sleep(0.5)

            # 5. æ•´åˆæ‰€æœ‰åµæ¸¬çµæœç‚ºå®Œæ•´ JSON
            yield f"data: {json.dumps({'log': 'ğŸ”— æ•´åˆæ‰€æœ‰åµæ¸¬çµæœ...'})}\n\n"
            
            # æ•´åˆå¾Œçš„å®Œæ•´åƒæ•¸
            integrated_result = {
                # å¾ classify_and_extract ä¾†çš„
                "category": classify_result.get("category", ""),
                "subcategory": classify_result.get("subcategory", []),
                "view_type": classify_result.get("view_type", []),
                "keywords": classify_result.get("keywords", []),
                "company_name": classify_result.get("company_name", ""),
                "event_type": classify_result.get("event_type", ""),
                
                # å¾ detect_stocks ä¾†çš„ï¼ˆå„ªå…ˆä½¿ç”¨ï¼‰
                "stock_id": stock_ids[0] if stock_ids else classify_result.get("stock_id", ""),
                "stock_ids": stock_ids,
                
                # å¾ detect_time ä¾†çš„ï¼ˆå„ªå…ˆä½¿ç”¨ï¼‰
                "time_info": time_info if time_info else classify_result.get("time_info", ""),
                
                # æ•´åˆå¾Œçš„å®Œæ•´è³‡è¨Š
                "question": question,
                "detection_timestamp": time.time()
            }
            
            yield f"data: {json.dumps({'log': 'ğŸ“‹ å®Œæ•´æ•´åˆçµæœ: ' + json.dumps(integrated_result, ensure_ascii=False, indent=2)})}\n\n"
            #time.sleep(0.5)

            # 6. åœ–è¡¨åµæ¸¬
            yield f"data: {json.dumps({'log': 'ğŸ“ˆ åœ–è¡¨åµæ¸¬ä¸­...'})}\n\n"
            chart_result = detect_chart(question)
            if chart_result:
                chart_type = chart_result.get("chart_type", "")
                table_id = chart_result.get("table_id", "")
                yield f"data: {json.dumps({'log': f'ğŸ“Š åµæ¸¬åˆ°åœ–è¡¨: {chart_type} (Table ID: {table_id})'})}\n\n"
            else:
                yield f"data: {json.dumps({'log': 'âŒ æœªåµæ¸¬åˆ°ç‰¹å®šåœ–è¡¨é¡å‹'})}\n\n"
            time.sleep(0.5)

            # 7. æ–°èæœå°‹
            yield f"data: {json.dumps({'log': 'ğŸ” é–‹å§‹æœå°‹ç›¸é—œæ–°è...'})}\n\n"
            serper_api_key = os.getenv("SERPER_API_KEY")
            if not serper_api_key:
                yield f"data: {json.dumps({'log': 'âš ï¸ è­¦å‘Š: æœªè¨­å®š SERPER_API_KEYï¼Œè·³éæ–°èæœå°‹'})}\n\n"
            else:
                # å¾æ•´åˆçµæœæå–æœå°‹é—œéµå­—
                company_name = integrated_result.get("company_name", "")
                stock_id = integrated_result.get("stock_id", "")
                keywords = integrated_result.get("keywords", [])
                category = integrated_result.get("category", "")
                subcategory = integrated_result.get("subcategory", [])
                view_type = integrated_result.get("view_type", [])
                time_info = integrated_result.get("time_info", "")
                
                # æ·»åŠ è©³ç´°çš„ console log ä¾†èª¿è©¦
                print(f"ğŸ” DEBUG - integrated_result: {integrated_result}")
                print(f"ğŸ” DEBUG - company_name: '{company_name}'")
                print(f"ğŸ” DEBUG - stock_id: '{stock_id}'")
                print(f"ğŸ” DEBUG - keywords: {keywords} (é•·åº¦: {len(keywords)})")
                print(f"ğŸ” DEBUG - category: '{category}'")
                print(f"ğŸ” DEBUG - subcategory: {subcategory} (é•·åº¦: {len(subcategory)})")
                print(f"ğŸ” DEBUG - view_type: {view_type} (é•·åº¦: {len(view_type)})")
                print(f"ğŸ” DEBUG - time_info: '{time_info}'")
                
                search_keywords = generate_smart_search_keywords(
                    category=category,
                    subcategory=subcategory,
                    view_type=view_type,
                    company_name=company_name,
                    stock_id=stock_id,
                    keywords=keywords,
                    time_info=time_info
                )
                
                print(f"ğŸ” DEBUG - æœ€çµ‚ search_keywords: {search_keywords} (é•·åº¦: {len(search_keywords)})")
                
                search_keywords_str = ', '.join(search_keywords)
                log_msg = f'ğŸ” æœå°‹é—œéµå­—: {search_keywords_str}'
                yield f"data: {json.dumps({'log': log_msg})}\n\n"
                
                try:
                    search_result = search_news_smart(
                        company_name=company_name,
                        stock_id=stock_id,
                        intent=integrated_result.get("category", ""),
                        keywords=search_keywords,
                        serper_api_key=serper_api_key,
                        use_grouped=True  # ä½¿ç”¨åˆ†çµ„æœå°‹
                    )
                    
                    if search_result.get("success"):
                        news_count = len(search_result.get("results", []))
                        yield f"data: {json.dumps({'log': f'ğŸ“° ç¬¬ä¸€æ¬¡æœå°‹æ‰¾åˆ° {news_count} å‰‡ç›¸é—œæ–°è'})}\n\n"
                        
                        # é¡¯ç¤ºæ¯å‰‡æ–°èæ¨™é¡Œ
                        for i, news in enumerate(search_result.get("results", [])[:5]):  # åªé¡¯ç¤ºå‰5å‰‡
                            title = news.get("title", "")
                            yield f"data: {json.dumps({'log': f'ğŸ“„ {i+1}. {title}'})}\n\n"
                            time.sleep(0.3)
                        
                        # ç¬¬äºŒæ¬¡æœå°‹ï¼šæ ¹æ“šç¬¬ä¸€æ¬¡æœå°‹çµæœç”Ÿæˆæ–°çš„é—œéµå­—
                        yield f"data: {json.dumps({'log': 'ğŸ”„ é–‹å§‹ç¬¬äºŒæ¬¡æœå°‹ï¼Œæ ¹æ“šç¬¬ä¸€æ¬¡çµæœç”Ÿæˆæ–°é—œéµå­—...'})}\n\n"
                        
                        # å¾ç¬¬ä¸€æ¬¡æœå°‹çµæœä¸­æå–æ–°çš„é—œéµå­—
                        first_search_results = search_result.get("results", [])
                        new_keywords = extract_keywords_from_results(first_search_results, company_name, stock_id)
                        
                        new_keywords_str = ", ".join(new_keywords)
                        yield f"data: {json.dumps({'log': f'ğŸ” ç¬¬äºŒæ¬¡æœå°‹é—œéµå­—: {new_keywords_str}'})}\n\n"
                        
                        # åŸ·è¡Œç¬¬äºŒæ¬¡æœå°‹
                        second_search_result = search_news_smart(
                            company_name=company_name,
                            stock_id=stock_id,
                            intent=integrated_result.get("category", ""),
                            keywords=new_keywords,
                            serper_api_key=serper_api_key,
                            use_grouped=True  # ä½¿ç”¨åˆ†çµ„æœå°‹
                        )
                        
                        if second_search_result.get("success"):
                            second_news_count = len(second_search_result.get("results", []))
                            yield f"data: {json.dumps({'log': f'ğŸ“° ç¬¬äºŒæ¬¡æœå°‹æ‰¾åˆ° {second_news_count} å‰‡ç›¸é—œæ–°è'})}\n\n"
                            
                            # åˆä½µå…©æ¬¡æœå°‹çµæœ
                            all_results = merge_search_results(first_search_results, second_search_result.get("results", []))
                            yield f"data: {json.dumps({'log': f'ğŸ“‹ åˆä½µå¾Œç¸½å…± {len(all_results)} å‰‡æ–°è'})}\n\n"
                            
                            # æ›´æ–° search_result ç‚ºåˆä½µå¾Œçš„çµæœ
                            search_result["results"] = all_results
                            
                            # æª¢æŸ¥æ˜¯å¦ç‚ºå€‹è‚¡åˆ†æé¡åˆ¥ï¼Œå¦‚æœæ˜¯å‰‡çˆ¬å–è²¡å‹™æ•¸æ“š
                            if "å€‹è‚¡åˆ†æ" in category:
                                # ç²å– Yahoo è²¡ç¶“è²¡å‹™å ±è¡¨æ•¸æ“š
                                yield f"data: {json.dumps({'log': 'ğŸ“Š æ­£åœ¨ç²å– Yahoo è²¡ç¶“è²¡å‹™å ±è¡¨æ•¸æ“š...'})}\n\n"
                                financial_data = fetch_yahoo_financial_data(stock_id, company_name)
                                
                                if financial_data.get("success"):
                                    data = financial_data.get("data", {})
                                    yield f"data: {json.dumps({'log': 'ğŸ“Š æˆåŠŸç²å–è²¡å‹™å ±è¡¨æ•¸æ“š'})}\n\n"
                                    
                                    # æº–å‚™è²¡å‹™æ•¸æ“šä¸Šä¸‹æ–‡
                                    financial_context = f"""
Yahoo è²¡ç¶“è²¡å‹™å ±è¡¨æ•¸æ“šï¼š

æ¯è‚¡ç›ˆé¤˜ (EPS)ï¼š
{format_eps_data(data.get('eps', {}))}

ç‡Ÿæ”¶æ•¸æ“šï¼š
{format_revenue_data(data.get('revenue', {}))}

æç›Šè¡¨æ•¸æ“šï¼š
{format_income_statement_data(data.get('income_statement', {}))}

è³‡ç”¢è² å‚µè¡¨æ•¸æ“šï¼š
{format_balance_sheet_data(data.get('balance_sheet', {}))}

è³‡æ–™ä¾†æºï¼š
{format_sources_data(data.get('sources', []))}
"""
                                    
                                    # æº–å‚™æ–°èä¾†æº
                                    news_sources = []
                                    for news in search_result.get("results", []):
                                        news_sources.append({
                                            "title": news.get("title", "ç„¡æ¨™é¡Œ"),
                                            "link": news.get("link", "")
                                        })
                                    
                                    # æº–å‚™è²¡å‹™è³‡æ–™ä¾†æº
                                    financial_sources = data.get('sources', [])
                                    
                                    # æ§‹å»ºæ–°èæ‘˜è¦
                                    news_summary = ""
                                    if search_result.get("results"):
                                        news_summary = "\n".join([
                                            f"{i+1}. {news.get('title', 'ç„¡æ¨™é¡Œ')}: {news.get('snippet', 'ç„¡æ‘˜è¦')}"
                                            for i, news in enumerate(search_result.get("results", [])[:5])
                                        ])
                                    
                                    # å„²å­˜è²¡å‹™æ•¸æ“šä¾›å¾ŒçºŒä½¿ç”¨
                                    financial_data = data
                                else:
                                    yield f"data: {json.dumps({'log': 'âš ï¸ ç„¡æ³•ç²å– Yahoo è²¡ç¶“è²¡å‹™å ±è¡¨æ•¸æ“š'})}\n\n"
                            else:
                                # éå€‹è‚¡åˆ†æé¡åˆ¥ï¼Œè·³éè²¡å‹™æ•¸æ“šç²å–
                                yield f"data: {json.dumps({'log': 'ğŸ“ éå€‹è‚¡åˆ†æé¡åˆ¥ï¼Œè·³éè²¡å‹™æ•¸æ“šç²å–'})}\n\n"
                        else:
                            yield f"data: {json.dumps({'log': 'âŒ ç¬¬äºŒæ¬¡æ–°èæœå°‹å¤±æ•—: ' + second_search_result.get('error', 'æœªçŸ¥éŒ¯èª¤')})}\n\n"
                    else:
                        yield f"data: {json.dumps({'log': 'âŒ æ–°èæœå°‹å¤±æ•—: ' + search_result.get('error', 'æœªçŸ¥éŒ¯èª¤')})}\n\n"
                except Exception as e:
                    yield f"data: {json.dumps({'log': f'âŒ æ–°èæœå°‹éŒ¯èª¤: {str(e)}'})}\n\n"
            
            time.sleep(0.5)

            # 8. æœ€çµ‚æŠ•è³‡åˆ†æå ±å‘Šç”Ÿæˆ
            yield f"data: {json.dumps({'log': 'ğŸ“ æ­£åœ¨ç”ŸæˆæŠ•è³‡åˆ†æå ±å‘Š...'})}\n\n"
            try:
                if 'search_result' in locals() and search_result and search_result.get("success"):
                    # æ§‹å»ºæ–°èæ‘˜è¦
                    news_summary = ""
                    if search_result.get("results"):
                        news_summary = "\n".join([
                            f"{i+1}. {news.get('title', 'ç„¡æ¨™é¡Œ')}: {news.get('snippet', 'ç„¡æ‘˜è¦')}"
                            for i, news in enumerate(search_result.get("results", [])[:5])
                        ])
                    
                    # æº–å‚™æ–°èä¾†æº
                    news_sources = []
                    for news in search_result.get("results", []):
                        news_sources.append({
                            "title": news.get("title", "ç„¡æ¨™é¡Œ"),
                            "link": news.get("link", "")
                        })
                    
                    # æº–å‚™è²¡å‹™ä¾†æº
                    financial_sources = []
                    if 'financial_data' in locals() and financial_data:
                        financial_sources.append({
                            "name": "Yahoo Finance",
                            "url": f"https://finance.yahoo.com/quote/{stock_id}.TW"
                        })
                    
                    # ä½¿ç”¨æ–°çš„ pipeline ç”Ÿæˆå ±å‘Š
                    summary_result = generate_report_pipeline(
                        company_name=company_name,
                        stock_id=stock_id,
                        intent=integrated_result.get("category", ""),
                        time_info=integrated_result.get("time_info", ""),
                        news_summary=news_summary,
                        news_sources=news_sources,
                        financial_data=financial_data if 'financial_data' in locals() else None,
                        financial_sources=financial_sources
                    )
                    
                    if summary_result.get("success"):
                        sections = summary_result.get("sections", [])
                        yield f"data: {json.dumps({'log': f'ğŸ“‹ ç”Ÿæˆ {len(sections)} å€‹åˆ†æé¢å‘'})}\n\n"
                        
                        # æ·»åŠ è©³ç´°çš„èª¿è©¦ä¿¡æ¯
                        print(f"[DEBUG] Sections é¡å‹: {type(sections)}")
                        print(f"[DEBUG] Sections å…§å®¹: {sections}")
                        
                        # é¡¯ç¤ºæ¯å€‹ section çš„æ¨™é¡Œ
                        for section in sections:
                            section_title = section.get("section", "æœªå‘½åå€å¡Š")
                            yield f"data: {json.dumps({'log': f'ğŸ“Š {section_title}'})}\n\n"
                            time.sleep(0.3)
                        
                        # ç™¼é€å®Œæ•´çš„æŠ•è³‡åˆ†æå ±å‘Š
                        print("[DEBUG] è™•ç† list æ ¼å¼çš„ sections")
                        report_sections = []
                        for section in sections:
                            # å¯é¸ï¼šè‡ªå‹•æ¨™è¨» typeï¼Œæ–¹ä¾¿å‰ç«¯æ¸²æŸ“
                            if section.get("cards"):
                                section["type"] = "cards"
                            elif section.get("tabs"):
                                section["type"] = "tabs"
                            elif section.get("bullets"):
                                section["type"] = "bullets"
                            elif section.get("sources"):
                                section["type"] = "sources"
                            elif section.get("disclaimer"):
                                section["type"] = "disclaimer"
                            elif section.get("summary_table"):
                                section["type"] = "summary_table"
                            report_sections.append(section)
                            print(f"[DEBUG] Section: {section.get('section', 'æœªå‘½åå€å¡Š')}")
                        report_data = {
                            'report': {
                                'stockName': company_name,
                                'stockId': stock_id,
                                'sections': report_sections,
                                'paraphrased_prompt': summary_result.get('paraphrased_prompt'),
                                'logs': summary_result.get('logs', [])
                            }
                        }
                        yield f"data: {json.dumps(report_data)}\n\n"
                    else:
                        yield f"data: {json.dumps({'log': 'âŒ æŠ•è³‡åˆ†æå ±å‘Šç”Ÿæˆå¤±æ•—: ' + summary_result.get('error', 'æœªçŸ¥éŒ¯èª¤')})}\n\n"
                else:
                    yield f"data: {json.dumps({'log': 'âš ï¸ ç„¡æ³•ç”ŸæˆæŠ•è³‡åˆ†æå ±å‘Šï¼Œç¼ºå°‘æ–°èè³‡æ–™'})}\n\n"
            except Exception as e:
                yield f"data: {json.dumps({'log': f'âŒ æŠ•è³‡åˆ†æå ±å‘Šç”ŸæˆéŒ¯èª¤: {str(e)}'})}\n\n"

            yield f"data: {json.dumps({'log': 'ğŸ‰ åˆ†ææµç¨‹å®Œæˆï¼'})}\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'log': f'âŒ ç³»çµ±éŒ¯èª¤: {str(e)}'})}\n\n"

    return StreamingResponse(event_stream(), media_type="text/event-stream")

@app.post("/api/investment-analysis")
async def investment_analysis_api(req: InvestmentAnalysisRequest):
    """
    åŸ·è¡Œå®Œæ•´çš„æŠ•è³‡åˆ†ææµç¨‹
    """
    return {
        "success": False,
        "error": "æ­¤ API å·²æ£„ç”¨ï¼Œè«‹ä½¿ç”¨ /api/ask-sse",
        "message": "è«‹ä½¿ç”¨ SSE ç«¯é»ä¾†ç²å–å¯¦æ™‚åˆ†æçµæœ"
    }

@app.get("/api/investment-analysis-sse")
def investment_analysis_sse_api(question: str = Query(...), serper_api_key: str = Query(None)):
    """
    ä½¿ç”¨ Server-Sent Events çš„æŠ•è³‡åˆ†ææµç¨‹
    """
    return ask_sse_api(question)

@app.post("/api/test-table-ids")
async def test_table_ids_api(req: DatabaseTestRequest):
    """
    æ¸¬è©¦å¤šå€‹ table_id æ˜¯å¦æœ‰æ•ˆ
    """
    results = db_query.test_table_ids(req.table_ids, req.stock_id)
    return {
        "success": True,
        "results": results,
        "tested_table_ids": req.table_ids,
        "tested_stock_id": req.stock_id
    }

@app.post("/api/query-database")
async def query_database_api(req: DatabaseQueryRequest):
    """
    æ ¹æ“šæ„åœ–å’Œè‚¡ç¥¨ä»£è™ŸæŸ¥è©¢è³‡æ–™åº«
    """
    result = db_query.query_data(
        req.intent_category,
        req.investment_aspect,
        req.stock_ids
    )
    return result

@app.post("/api/query-chart")
async def query_chart_api(req: ChartQueryRequest):
    """
    æ ¹æ“šåœ–è¡¨é¡å‹å’Œè‚¡ç¥¨ä»£è™ŸæŸ¥è©¢è³‡æ–™
    """
    from langgraph_app.nodes.detect_chart import CHART_MAPPING
    
    if req.chart_type not in CHART_MAPPING:
        return {
            "success": False,
            "error": f"ä¸æ”¯æ´çš„åœ–è¡¨é¡å‹: {req.chart_type}"
        }
    
    config = CHART_MAPPING[req.chart_type]
    
    # å»ºç«‹ API URL
    from langgraph_app.data_tools.template_mapping import build_api_url
    url = build_api_url(config["table_id"], req.stock_ids, "MTPeriod=0;DTMode=0;DTRange=5;DTOrder=1;MajorTable=M173;")
    
    # ç™¼é€è«‹æ±‚
    import requests
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
        "Referer": "https://www.cmoney.tw/",
        "Accept": "application/json",
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            return {
                "success": True,
                "data": data,
                "chart_type": req.chart_type,
                "table_id": config["table_id"],
                "url": url,
                "stock_ids": req.stock_ids
            }
        else:
            return {
                "success": False,
                "error": f"API è«‹æ±‚å¤±æ•—: {response.status_code}"
            }
    except Exception as e:
        return {
            "success": False,
            "error": f"è«‹æ±‚éŒ¯èª¤: {str(e)}"
        }

@app.get("/api/test-known-tables")
async def test_known_tables_api():
    """
    æ¸¬è©¦å·²çŸ¥çš„ table_id åˆ—è¡¨
    """
    known_table_ids = [
        "105567992",  # æ–°è
        "105567993",  # ç±Œç¢¼
        "105567994",  # åŸºæœ¬é¢
        "105567995",  # æŠ€è¡“æ¯”è¼ƒ
        "105567996",  # ç±Œç¢¼æ¯”è¼ƒ
        "105567997",  # åŸºæœ¬é¢æ¯”è¼ƒ
        "105567998",  # å¤§ç›¤æŠ€è¡“
        "105567999",  # å¤§ç›¤ç±Œç¢¼
        "105568000",  # ç”¢æ¥­æŠ€è¡“
        "105568001",  # ç”¢æ¥­ç±Œç¢¼
    ]
    
    results = db_query.test_table_ids(known_table_ids)
    return {
        "success": True,
        "results": results,
        "tested_count": len(known_table_ids)
    }

@app.websocket("/ws/ask")
async def websocket_ask(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        data = await websocket.receive_text()
        req = json.loads(data)
        question = req.get("question", "")
        
        # ç™¼é€é–‹å§‹åµæ¸¬çš„è¨Šæ¯
        await manager.send_message("ğŸ” é–‹å§‹åµæ¸¬è‚¡ç¥¨ä»£è™Ÿ...", websocket)
        
        # åµæ¸¬è‚¡ç¥¨ä»£è™Ÿ
        stock_ids = detect_stocks(question)
        
        # ç™¼é€åµæ¸¬çµæœ
        if stock_ids:
            await manager.send_message(f"âœ… åµæ¸¬åˆ°è‚¡ç¥¨ä»£è™Ÿï¼š{', '.join(stock_ids)}", websocket)
        else:
            await manager.send_message("âŒ æœªåµæ¸¬åˆ°è‚¡ç¥¨ä»£è™Ÿ", websocket)
            
    except WebSocketDisconnect:
        manager.disconnect(websocket)
    except Exception as e:
        await manager.send_message(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}", websocket)
    finally:
        manager.disconnect(websocket)
        await websocket.close()

def generate_smart_search_keywords(category: str, subcategory: List[str], view_type: List[str], company_name: str, stock_id: str, keywords: List[str], time_info: str = "") -> List[str]:
    """
    æ ¹æ“šä¸åŒçš„åˆ†é¡å’Œå­åˆ†é¡æ™ºèƒ½ç”Ÿæˆæœå°‹é—œéµå­—ï¼ŒåŒ…å«ç¶²ç«™é™åˆ¶
    """
    try:
        # ä½¿ç”¨ search_news æ¨¡çµ„ä¸­çš„æ–°é‚è¼¯
        from langgraph_app.nodes.search_news import generate_search_keywords
        return generate_search_keywords(company_name, stock_id, category, keywords, "", time_info)
    except Exception as e:
        print(f"[generate_smart_search_keywords ERROR] {e}")
        # å¦‚æœæ–°é‚è¼¯å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨é‚è¼¯
        return generate_fallback_smart_keywords(category, subcategory, view_type, company_name, stock_id, keywords, time_info)

def generate_fallback_smart_keywords(category: str, subcategory: List[str], view_type: List[str], company_name: str, stock_id: str, keywords: List[str], time_info: str = "") -> List[str]:
    """
    å‚™ç”¨çš„æ™ºèƒ½æœå°‹é—œéµå­—ç”Ÿæˆé‚è¼¯ï¼Œå……åˆ†åˆ©ç”¨æ‰€æœ‰å…è¨±çš„ç¶²ç«™
    """
    search_keywords = []
    
    # å®šç¾©å…è¨±çš„ç¶²ç«™
    allowed_sites = [
        "tw.finance.yahoo.com",  # Yahooå¥‡æ‘©è‚¡å¸‚
        "cnyes.com",  # é‰…äº¨ç¶²
        "moneydj.com",  # MoneyDJ ç†è²¡ç¶²
        "cmoney.tw",  # CMoney
        "money.udn.com",  # ç¶“æ¿Ÿæ—¥å ±
        "ctee.com.tw",  # å·¥å•†æ™‚å ±
        "finance.ettoday.net",  # ETtoday è²¡ç¶“
        "goodinfo.tw",  # Goodinfo
        "macromicro.me",  # è²¡ç¶“Må¹³æ–¹
        "smart.businessweekly.com.tw",  # Smartæ™ºå¯Œ
        "technews.tw",  # ç§‘æŠ€æ–°å ±
        "nownews.com",  # Nownews
        "moneylink.com.tw",  # MoneyLink å¯Œè¯ç¶²
        "stockfeel.com.tw",  # è‚¡æ„Ÿ StockFeel
        "businessweekly.com.tw",  # å•†æ¥­å‘¨åˆŠ
        "businesstoday.com.tw",  # ä»Šå‘¨åˆŠ
        "pchome.com.tw",  # PChome è‚¡å¸‚é »é“
    ]
    
    # åŸºç¤é—œéµå­—çµ„åˆ - å……åˆ†åˆ©ç”¨æ‰€æœ‰ä¸»è¦ç¶²ç«™
    if company_name and stock_id:
        search_keywords.extend([
            f"{company_name} {stock_id} è²¡å ± site:tw.finance.yahoo.com",
            f"{company_name} å¤–è³‡è²·è³£ site:cnyes.com",
            f"{stock_id} æ³•äººå‹•å‘ site:moneydj.com",
            f"{company_name} EPS åˆ†æ site:cmoney.tw",
            f"{company_name} è²¡ç¶“æ–°è site:money.udn.com",
            f"{stock_id} å·¥å•†æ™‚å ± site:ctee.com.tw",
            f"{company_name} è²¡ç¶“å ±å° site:finance.ettoday.net",
            f"{company_name} åŸºæœ¬é¢ site:goodinfo.tw",
            f"{company_name} ç¸½é«”ç¶“æ¿Ÿ site:macromicro.me",
            f"{company_name} æŠ•è³‡ç†è²¡ site:smart.businessweekly.com.tw",
            f"{company_name} ç§‘æŠ€æ–°è site:technews.tw",
            f"{company_name} å³æ™‚æ–°è site:nownews.com"
        ])
    
    # æ ¹æ“šå¤§åˆ†é¡ç”Ÿæˆé—œéµå­—
    if category == "å€‹è‚¡åˆ†æ":
        if company_name and stock_id:
            search_keywords.extend([
                f"{company_name} ç‡Ÿæ”¶ æ¯›åˆ©ç‡ site:cmoney.tw",
                f"{stock_id} ä¸‰å¤§æ³•äºº site:goodinfo.tw",
                f"{company_name} è²¡å‹™åˆ†æ site:tw.finance.yahoo.com"
            ])
            
        # æ ¹æ“šå­åˆ†é¡é€²ä¸€æ­¥ç´°åŒ–
        for sub in subcategory:
            if "åŸºæœ¬é¢åˆ†æ" in sub:
                search_keywords.extend([
                    f"{company_name} è²¡å‹™åˆ†æ site:tw.finance.yahoo.com",
                    f"{stock_id} æç›Šè¡¨ site:goodinfo.tw",
                    f"{company_name} ç‡Ÿæ”¶åˆ†æ site:cmoney.tw"
                ])
            elif "ç±Œç¢¼é¢åˆ†æ" in sub:
                search_keywords.extend([
                    f"{stock_id} ç±Œç¢¼åˆ†æ site:cnyes.com",
                    f"{company_name} å¤–è³‡æŒè‚¡ site:moneydj.com",
                    f"{stock_id} æŠ•ä¿¡å‹•å‘ site:goodinfo.tw"
                ])
            elif "æŠ€è¡“é¢åˆ†æ" in sub:
                search_keywords.extend([
                    f"{company_name} æŠ€è¡“åˆ†æ site:moneydj.com",
                    f"{stock_id} æŠ€è¡“ç·šåœ– site:cmoney.tw",
                    f"{company_name} æŠ€è¡“æŒ‡æ¨™ site:goodinfo.tw"
                ])
                
    elif category == "é¸è‚¡å»ºè­°":
        if company_name:
            search_keywords.extend([
                f"{company_name} é¸è‚¡å»ºè­° site:cmoney.tw",
                f"{stock_id} æŠ•è³‡æ¨™çš„ site:goodinfo.tw",
                f"{company_name} æŠ•è³‡åˆ†æ site:smart.businessweekly.com.tw"
            ])
                
    elif category == "ç›¤å‹¢åˆ†æ":
        search_keywords.extend([
            "å°è‚¡ å¤§ç›¤åˆ†æ site:cnyes.com",
            "å°è‚¡ ç›¤å‹¢ site:moneydj.com",
            "å°è‚¡ å¤§ç›¤æŠ€è¡“é¢ site:tw.finance.yahoo.com"
        ])
        
        for sub in subcategory:
            if "ç”¢æ¥­" in sub:
                search_keywords.extend([
                    "ç”¢æ¥­åˆ†æ site:tw.finance.yahoo.com",
                    "ç”¢æ¥­è¼ªå‹• site:cnyes.com",
                    "ç”¢æ¥­è¶¨å‹¢ site:technews.tw"
                ])
            elif "åœ‹éš›è‚¡å¸‚" in sub:
                search_keywords.extend([
                    "åœ‹éš›è‚¡å¸‚ site:cnyes.com",
                    "ç¾è‚¡ å°è‚¡ site:tw.finance.yahoo.com",
                    "åœ‹éš›æƒ…å‹¢ site:macromicro.me"
                ])
                
    elif category == "æ¯”è¼ƒåˆ†æ":
        if company_name:
            search_keywords.extend([
                f"{company_name} æ¯”è¼ƒåˆ†æ site:cmoney.tw",
                f"{company_name} åŒæ¥­æ¯”è¼ƒ site:goodinfo.tw",
                f"{stock_id} ç«¶çˆ­å°æ‰‹ site:cnyes.com"
            ])
    
    # æ·»åŠ æ™‚é–“ç›¸é—œé—œéµå­—
    if time_info and time_info != "recent_5_days":
        time_keyword = ""
        if "today" in time_info:
            time_keyword = "ä»Šå¤©"
        elif "yesterday" in time_info:
            time_keyword = "æ˜¨å¤©"
        elif "this_week" in time_info:
            time_keyword = "æœ¬é€±"
        elif "this_month" in time_info:
            time_keyword = "æœ¬æœˆ"
        elif "this_quarter" in time_info:
            time_keyword = "æœ¬å­£"
        elif "this_year" in time_info:
            time_keyword = "ä»Šå¹´"
        
        if time_keyword and company_name:
            search_keywords.extend([
                f"{company_name} {time_keyword} æ–°è site:cnyes.com",
                f"{stock_id} {time_keyword} å ±å° site:money.udn.com",
                f"{company_name} {time_keyword} åˆ†æ site:finance.ettoday.net"
            ])
    
    # æ·»åŠ æŠ•è³‡é¢å‘ç›¸é—œé—œéµå­—
    for view in view_type:
        if "åŸºæœ¬é¢" in view and company_name:
            search_keywords.extend([
                f"{company_name} åŸºæœ¬é¢ site:tw.finance.yahoo.com",
                f"{stock_id} è²¡å‹™é¢ site:goodinfo.tw",
                f"{company_name} ç‡Ÿæ”¶é¢ site:cmoney.tw"
            ])
        elif "æŠ€è¡“é¢" in view and company_name:
            search_keywords.extend([
                f"{company_name} æŠ€è¡“é¢ site:moneydj.com",
                f"{stock_id} æŠ€è¡“åˆ†æ site:cmoney.tw",
                f"{company_name} æŠ€è¡“æŒ‡æ¨™ site:goodinfo.tw"
            ])
        elif "ç±Œç¢¼é¢" in view and stock_id:
            search_keywords.extend([
                f"{stock_id} ç±Œç¢¼é¢ site:goodinfo.tw",
                f"{company_name} æ³•äººé¢ site:cnyes.com",
                f"{stock_id} å¤–è³‡é¢ site:moneydj.com"
            ])
    
    # æ·»åŠ å¹´ä»½ç›¸é—œé—œéµå­—
    current_year = "2025"
    last_year = "2024"
    if company_name:
        search_keywords.extend([
            f"{company_name} {current_year} è²¡å ± site:tw.finance.yahoo.com",
            f"{company_name} {last_year} æç›Šè¡¨ site:cnyes.com",
            f"{stock_id} {current_year} æ³•äººå‹•å‘ site:moneydj.com"
        ])
    
    # é™åˆ¶æ•¸é‡ä¸¦ç¢ºä¿ä¸é‡è¤‡
    unique_keywords = list(dict.fromkeys(search_keywords))
    return unique_keywords[:12]  # å¢åŠ åˆ°æœ€å¤š12å€‹é—œéµå­—

def extract_keywords_from_results(search_results: List[Dict], company_name: str, stock_id: str) -> List[str]:
    """
    å¾ç¬¬ä¸€æ¬¡æœå°‹çµæœä¸­æå–æ–°çš„é—œéµå­—ï¼ŒåŒ…å«ç¶²ç«™é™åˆ¶
    """
    try:
        # æ”¶é›†æ‰€æœ‰æ¨™é¡Œå’Œæ‘˜è¦
        all_text = ""
        for result in search_results:
            title = result.get("title", "")
            snippet = result.get("snippet", "")
            all_text += f"{title} {snippet} "
        
        # ä½¿ç”¨ OpenAI å¾æœå°‹çµæœä¸­æå–æ–°çš„é—œéµå­—
        client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        prompt = f"""
æ ¹æ“šä»¥ä¸‹æœå°‹çµæœï¼Œç‚º {company_name}({stock_id}) ç”Ÿæˆ 3-5 å€‹æ–°çš„æœå°‹é—œéµå­—ã€‚
é€™äº›é—œéµå­—æ‡‰è©²èƒ½å¤ æ‰¾åˆ°æ›´æ·±å…¥ã€æ›´å…·é«”çš„ç›¸é—œè³‡è¨Šï¼Œç‰¹åˆ¥æ˜¯è²¡å‹™æ•¸æ“šã€è²¡å ±ã€æç›Šè¡¨ç­‰ã€‚

âš ï¸é™åˆ¶ä¾†æºï¼šè«‹åƒ…å¾ä¸‹åˆ—ç¶²ç«™ä¸­æŠ“å–å…§å®¹ï¼š
Yahooå¥‡æ‘©è‚¡å¸‚ã€é‰…äº¨ç¶² (cnyes)ã€MoneyDJ ç†è²¡ç¶²ã€CMoneyã€ç¶“æ¿Ÿæ—¥å ±ã€å·¥å•†æ™‚å ±ã€ETtoday è²¡ç¶“ã€Goodinfoã€è²¡ç¶“Må¹³æ–¹ï¼ˆMacroMicroï¼‰ã€Smartæ™ºå¯Œã€ç§‘æŠ€æ–°å ±ã€Nownewsã€MoneyLink å¯Œè¯ç¶²ã€è‚¡æ„Ÿ StockFeelã€å•†æ¥­å‘¨åˆŠã€ä»Šå‘¨åˆŠã€PChome è‚¡å¸‚é »é“ã€‚

æœå°‹çµæœï¼š
{all_text}

è«‹ç”Ÿæˆæ–°çš„æœå°‹é—œéµå­—ï¼Œæ ¼å¼ç‚º JSON é™£åˆ—ï¼Œä¸¦åŒ…å« site: é™åˆ¶ï¼š
["{{ company_name }} 2025 è²¡å ± site:tw.finance.yahoo.com", "{{ stock_id }} æ³•äººå‹•å‘ site:cnyes.com", "{{ company_name }} EPS åˆ†æ site:moneydj.com"]

æ³¨æ„ï¼šè«‹åŒ…å«å¹´ä»½(2025/2024)å’Œå…·é«”çš„ç¶²ç«™é™åˆ¶ã€‚
"""
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )
        
        # è§£æå›æ‡‰
        content = response.choices[0].message.content.strip()
        try:
            import json
            keywords = json.loads(content)
            if isinstance(keywords, list):
                return keywords[:5]  # é™åˆ¶æœ€å¤š5å€‹é—œéµå­—
        except:
            pass
        
        # å¦‚æœ AI è§£æå¤±æ•—ï¼Œä½¿ç”¨é è¨­é—œéµå­—
        return generate_fallback_second_keywords(company_name, stock_id)
        
    except Exception as e:
        print(f"[extract_keywords_from_results ERROR] {e}")
        # è¿”å›é è¨­é—œéµå­—ï¼ˆåŒ…å«å¹´ä»½å’Œè²¡å‹™é—œéµå­—ï¼‰
        return generate_fallback_second_keywords(company_name, stock_id)

def generate_fallback_second_keywords(company_name: str, stock_id: str) -> List[str]:
    """ç”Ÿæˆç¬¬äºŒæ¬¡æœå°‹çš„å‚™ç”¨é—œéµå­—ï¼Œå……åˆ†åˆ©ç”¨æ‰€æœ‰å…è¨±çš„ç¶²ç«™"""
    current_year = "2025"
    last_year = "2024"
    
    fallback_keywords = [
        f"{company_name} {current_year} è²¡å ± site:tw.finance.yahoo.com",
        f"{stock_id} {last_year} æç›Šè¡¨ site:cnyes.com", 
        f"{company_name} æ³•äººå‹•å‘ site:moneydj.com",
        f"{stock_id} EPS åˆ†æ site:cmoney.tw",
        f"{company_name} ç‡Ÿæ¥­æ”¶å…¥ site:goodinfo.tw",
        f"{company_name} è²¡ç¶“æ–°è site:money.udn.com",
        f"{stock_id} å·¥å•†æ™‚å ± site:ctee.com.tw",
        f"{company_name} è²¡ç¶“å ±å° site:finance.ettoday.net",
        f"{company_name} åŸºæœ¬é¢åˆ†æ site:macromicro.me",
        f"{company_name} æŠ•è³‡ç†è²¡ site:smart.businessweekly.com.tw",
        f"{company_name} ç§‘æŠ€æ–°è site:technews.tw",
        f"{company_name} å³æ™‚æ–°è site:nownews.com"
    ]
    
    return fallback_keywords[:12]  # å¢åŠ åˆ°æœ€å¤š12å€‹é—œéµå­—

def merge_search_results(first_results: List[Dict], second_results: List[Dict]) -> List[Dict]:
    """
    åˆä½µå…©æ¬¡æœå°‹çµæœï¼Œå»é™¤é‡è¤‡
    """
    try:
        # ä½¿ç”¨ URL ä½œç‚ºå»é‡ä¾æ“š
        seen_urls = set()
        merged_results = []
        
        # å…ˆåŠ å…¥ç¬¬ä¸€æ¬¡æœå°‹çµæœ
        for result in first_results:
            url = result.get("link", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                merged_results.append(result)
        
        # å†åŠ å…¥ç¬¬äºŒæ¬¡æœå°‹çµæœï¼ˆå»é‡ï¼‰
        for result in second_results:
            url = result.get("link", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                merged_results.append(result)
        
        return merged_results
        
    except Exception as e:
        print(f"[merge_search_results ERROR] {e}")
        # å¦‚æœåˆä½µå¤±æ•—ï¼Œè¿”å›ç¬¬ä¸€æ¬¡æœå°‹çµæœ
        return first_results

def fetch_yahoo_financial_data(stock_id: str, company_name: str) -> Dict:
    """
    å¾ FinLab API å–å¾—è²¡å‹™å ±è¡¨æ•¸æ“šï¼Œå¤±æ•—å‰‡ fallback ç”¨æ¨¡æ“¬è³‡æ–™
    """
    try:
        import finlab
        from finlab import data
        import pandas as pd
        
        # ç™»å…¥ FinLab API
        api_key = os.environ['FINLAB_API_KEY']
        finlab.login(api_token=api_key)
        
        print(f"[DEBUG] ä½¿ç”¨ FinLab API å–å¾— {stock_id} è²¡å‹™è³‡æ–™")
        
        # åˆå§‹åŒ–è³‡æ–™çµæ§‹
        eps_data = {}
        revenue_data = {}
        income_statement_data = {}
        balance_sheet_data = {}
        sources = []
        
        # åªå–ç¢ºå®šå¯ä»¥å–å¾—çš„è³‡æ–™
        finlab_data = {
            "æ¯è‚¡ç›ˆé¤˜": "financial_statement:æ¯è‚¡ç›ˆé¤˜",
            "ç‡Ÿæ¥­æ¯›åˆ©ç‡": "fundamental_features:ç‡Ÿæ¥­æ¯›åˆ©ç‡", 
            "æœˆç‡Ÿæ”¶æˆé•·ç‡": "monthly_revenue:å»å¹´åŒæœˆå¢æ¸›(%)"
        }
        
        for name, data_type in finlab_data.items():
            try:
                df = data.get(data_type)
                if stock_id in df.columns:
                    series = df[stock_id].dropna()
                    
                    # æ ¹æ“šä¸åŒçš„è³‡æ–™é¡å‹è™•ç† date index
                    if name == "æœˆç‡Ÿæ”¶æˆé•·ç‡":
                        # æœˆç‡Ÿæ”¶æˆé•·ç‡çš„ date index æ˜¯æœˆä»½æ ¼å¼
                        for date, value in series.tail(8).items():
                            # å°‡æœˆä»½æ ¼å¼è½‰æ›ç‚ºå­£åº¦æ ¼å¼
                            if isinstance(date, str):
                                # å¦‚æœæ˜¯å­—ä¸²æ ¼å¼ï¼Œå˜—è©¦è§£æ
                                try:
                                    from datetime import datetime
                                    if len(date) == 6:  # YYYYMM æ ¼å¼
                                        year = date[:4]
                                        month = int(date[4:6])
                                        quarter = f"{year}-Q{(month-1)//3 + 1}"
                                    else:
                                        quarter = str(date)
                                except:
                                    quarter = str(date)
                            else:
                                # å¦‚æœæ˜¯ datetime æ ¼å¼
                                quarter = f"{date.year}-Q{(date.month-1)//3 + 1}"
                            
                            if quarter not in income_statement_data:
                                income_statement_data[quarter] = {}
                            income_statement_data[quarter][name] = round(float(value), 2)
                    else:
                        # å…¶ä»–è³‡æ–™ä½¿ç”¨åŸæœ¬çš„å­£åº¦æ ¼å¼
                        for date, value in series.tail(4).items():
                            quarter = date
                            if quarter not in income_statement_data:
                                income_statement_data[quarter] = {}
                            
                            if name == "æ¯è‚¡ç›ˆé¤˜":
                                income_statement_data[quarter][name] = round(float(value), 2)
                            else:
                                income_statement_data[quarter][name] = round(float(value), 2)
                    
                    sources.append({"name": f"FinLab API - {name}", "url": "https://ai.finlab.tw/database"})
                    print(f"[DEBUG] å–å¾— {name} è³‡æ–™: {len(series.tail(8 if name == 'æœˆç‡Ÿæ”¶æˆé•·ç‡' else 4))} ç­†")
                    
            except Exception as e:
                print(f"[DEBUG] {name} è³‡æ–™å–å¾—å¤±æ•—: {e}")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å–å¾—ä»»ä½•è³‡æ–™
        if income_statement_data:
            print(f"[DEBUG] FinLab API æˆåŠŸå–å¾— {stock_id} è²¡å‹™è³‡æ–™")
            return {
                "eps": eps_data,
                "revenue": revenue_data,
                "income_statement": income_statement_data,
                "balance_sheet": balance_sheet_data,
                "sources": sources
            }
        else:
            print(f"[DEBUG] FinLab API æœªå–å¾—ä»»ä½•è³‡æ–™ï¼Œä½¿ç”¨ fallback")
            raise Exception("No data retrieved from FinLab API")
            
    except Exception as e:
        print(f"[DEBUG] FinLab API å¤±æ•—ï¼Œä½¿ç”¨ fallback è³‡æ–™: {e}")
        
        # Fallback æ¨¡æ“¬è³‡æ–™
        return {
            "eps": {
                "2024-Q1": 1.25,
                "2024-Q2": 1.45,
                "2024-Q3": 1.35,
                "2024-Q4": 1.55
            },
            "revenue": {
                "2024-Q1": 50000000000,
                "2024-Q2": 52000000000,
                "2024-Q3": 48000000000,
                "2024-Q4": 55000000000
            },
            "income_statement": {
                "2024-Q1": {"æ¯è‚¡ç›ˆé¤˜": 1.25, "ç‡Ÿæ”¶": 50000000000, "ç‡Ÿæ¥­åˆ©ç›Š": 8000000000},
                "2024-Q2": {"æ¯è‚¡ç›ˆé¤˜": 1.45, "ç‡Ÿæ”¶": 52000000000, "ç‡Ÿæ¥­åˆ©ç›Š": 8500000000},
                "2024-Q3": {"æ¯è‚¡ç›ˆé¤˜": 1.35, "ç‡Ÿæ”¶": 48000000000, "ç‡Ÿæ¥­åˆ©ç›Š": 7800000000},
                "2024-Q4": {"æ¯è‚¡ç›ˆé¤˜": 1.55, "ç‡Ÿæ”¶": 55000000000, "ç‡Ÿæ¥­åˆ©ç›Š": 9000000000}
            },
            "balance_sheet": {
                "2024-Q1": {"æ¯è‚¡æ·¨å€¼": 45.2},
                "2024-Q2": {"æ¯è‚¡æ·¨å€¼": 46.1},
                "2024-Q3": {"æ¯è‚¡æ·¨å€¼": 47.3},
                "2024-Q4": {"æ¯è‚¡æ·¨å€¼": 48.5}
            },
            "sources": [{"name": "æ¨¡æ“¬è³‡æ–™", "url": "fallback"}]
        }

def format_eps_data(eps_data: Dict) -> str:
    """æ ¼å¼åŒ– EPS æ•¸æ“š"""
    if not eps_data:
        return "ç„¡æ•¸æ“š"
    
    formatted = []
    for quarter, data in eps_data.items():
        eps = data.get('eps', 'N/A')
        q_growth = data.get('quarterly_growth', 'N/A')
        y_growth = data.get('yearly_growth', 'N/A')
        avg_price = data.get('avg_price', 'N/A')
        
        formatted.append(f"- {quarter}: EPS {eps} å…ƒ (å­£å¢ç‡: {q_growth}, å¹´å¢ç‡: {y_growth}, å­£å‡åƒ¹: {avg_price} å…ƒ)")
    
    return "\n".join(formatted)

def format_revenue_data(revenue_data: Dict) -> str:
    """æ ¼å¼åŒ–ç‡Ÿæ”¶æ•¸æ“š"""
    if not revenue_data:
        return "ç„¡æ•¸æ“š"
    
    formatted = []
    for quarter, data in revenue_data.items():
        revenue = data.get('revenue', 'N/A')
        q_growth = data.get('quarterly_growth', 'N/A')
        y_growth = data.get('yearly_growth', 'N/A')
        
        formatted.append(f"- {quarter}: ç‡Ÿæ”¶ {revenue} ä»Ÿå…ƒ (å­£å¢ç‡: {q_growth}, å¹´å¢ç‡: {y_growth})")
    
    return "\n".join(formatted)

def format_income_statement_data(income_data: Dict) -> str:
    """æ ¼å¼åŒ–æç›Šè¡¨æ•¸æ“š"""
    if not income_data:
        return "ç„¡æ•¸æ“š"
    
    formatted = []
    for item, quarters in income_data.items():
        item_name = {
            'revenue': 'ç‡Ÿæ¥­æ”¶å…¥',
            'gross_profit': 'ç‡Ÿæ¥­æ¯›åˆ©',
            'operating_income': 'ç‡Ÿæ¥­åˆ©ç›Š',
            'net_income': 'ç¨…å¾Œæ·¨åˆ©'
        }.get(item, item)
        
        quarter_data = []
        for quarter, value in quarters.items():
            quarter_data.append(f"{quarter}: {value} ä»Ÿå…ƒ")
        
        formatted.append(f"- {item_name}: {', '.join(quarter_data)}")
    
    return "\n".join(formatted)

def format_balance_sheet_data(balance_data: Dict) -> str:
    """æ ¼å¼åŒ–è³‡ç”¢è² å‚µè¡¨æ•¸æ“š"""
    if not balance_data:
        return "ç„¡æ•¸æ“š"
    
    formatted = []
    for item, quarters in balance_data.items():
        item_name = {
            'total_assets': 'è³‡ç”¢ç¸½è¨ˆ',
            'total_liabilities': 'è² å‚µç¸½è¨ˆ',
            'equity': 'è‚¡æ±æ¬Šç›Šç¸½è¨ˆ'
        }.get(item, item)
        
        quarter_data = []
        for quarter, value in quarters.items():
            quarter_data.append(f"{quarter}: {value} ä»Ÿå…ƒ")
        
        formatted.append(f"- {item_name}: {', '.join(quarter_data)}")
    
    return "\n".join(formatted)

def format_sources_data(sources: List[Dict]) -> str:
    """æ ¼å¼åŒ–è³‡æ–™ä¾†æº"""
    if not sources or not isinstance(sources, list):
        return "ç„¡è³‡æ–™ä¾†æº"
    
    formatted = []
    for source in sources:
        if isinstance(source, dict):
            formatted.append(f"- {source.get('name', 'æœªçŸ¥')}: {source.get('url', 'ç„¡é€£çµ')}")
        else:
            formatted.append(f"- {str(source)}")
    
    return "\n".join(formatted)

@app.post("/api/proxy_login")
async def proxy_login(request: Request):
    form = await request.form()
    url = "https://www.cmoney.tw/identity/token"
    resp = requests.post(url, data=form)
    return Response(content=resp.content, status_code=resp.status_code, media_type=resp.headers.get("Content-Type"))

@app.post("/api/proxy_custom_group")
async def proxy_custom_group(request: Request):
    """Proxy for CMoney CustomGroup API"""
    try:
        # è®€å– request body
        body = await request.body()
        
        # è½‰ç™¼åˆ° CMoney API
        url = "https://www.cmoney.tw/MobileService/ashx/CustomerGroup/CustomGroup.ashx"
        headers = {
            'Content-Type': 'application/x-www-form-urlencoded',
        }
        
        # å¦‚æœæœ‰ Authorization headerï¼Œä¹Ÿè¦è½‰ç™¼
        auth_header = request.headers.get('Authorization')
        if auth_header:
            headers['Authorization'] = auth_header
            
        resp = requests.post(url, data=body, headers=headers)
        
        return Response(
            content=resp.content, 
            status_code=resp.status_code, 
            media_type=resp.headers.get("Content-Type", "application/json")
        )
    except Exception as e:
        return Response(
            content=json.dumps({"error": str(e)}), 
            status_code=500, 
            media_type="application/json"
        )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 